{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891593b9",
   "metadata": {},
   "source": [
    "# Implement sentiment analysis using logistic regression\n",
    "\n",
    "   <div style=\"margin-left: 20px; margin-top:10px\">\n",
    "       The following steps outline the implementation of sentiment analysis using the Natural \n",
    "        Language Toolkit (NLTK) library, demonstrated with the 'Twitter_samples' from the NLTK corpus.\n",
    "            <ul>\n",
    "                <li>Import Necessary Libraries;</li>\n",
    "                <li>Prepare the Data;</li>\n",
    "                <li>Utils implemenration;</li>\n",
    "                <li>Split the Data into Training and Test Sets;</li>\n",
    "                <li>Model implementation;</li>\n",
    "            </ul>\n",
    "   </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef8675",
   "metadata": {},
   "source": [
    "## Step 1: Import Necessary Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d45ce5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/musa24/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:143: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.19.2)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: colorama in /Users/musa24/opt/anaconda3/lib/python3.8/site-packages (0.4.5)\r\n"
     ]
    }
   ],
   "source": [
    "import nltk  # natural language toolkit\n",
    "from nltk.corpus import twitter_samples  # contains the twitter dataset\n",
    "from nltk.corpus import stopwords        # stopwords of the df languages\n",
    "from nltk.stem import PorterStemmer      # word stemming\n",
    "from nltk.tokenize import TweetTokenizer  # Tokenizing\n",
    "import numpy as np\n",
    "import string\n",
    "import re     # regular regression\n",
    "\n",
    "from colorama import Fore   # coloring\n",
    "\n",
    "! pip install colorama  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5773d4f2",
   "metadata": {},
   "source": [
    "## Step 2 : Prepare the Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b720543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\u001b[31m  I have a really good m&amp;g idea but I'm never going to meet them :(((\n"
     ]
    }
   ],
   "source": [
    "all_positive_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
    "all_negative_tweets = twitter_samples.strings(\"negative_tweets.json\")\n",
    "\n",
    "\n",
    "print(Fore.GREEN ,f\"{all_positive_tweets[0]}\")\n",
    "print(Fore.RED , f\" {all_negative_tweets[10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91fbbe1",
   "metadata": {},
   "source": [
    "## Step 3: Utils implementation.    \n",
    "   <div style=\"margin-left: 20px; margin-top:10px\">\n",
    "        The following are the functions that help with model training and evaluation.\n",
    "            <ul>\n",
    "                <li>Process Tweet;</li>\n",
    "                <li>Word Frequencies;</li>\n",
    "                <li>Sigmoid Function;</li>\n",
    "                <li>Instances of feature extraction;</li>\n",
    "                <li>Gradient Descent;</li>\n",
    "                <li>Predict Tweet;</li>\n",
    "            </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6c9dd",
   "metadata": {},
   "source": [
    "### Step 3.1: Process Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d5796c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement process_tweet(tweet)\n",
    "english_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    '''\n",
    "     - Removing handles , URLS, Hash(#) and extra space\n",
    "     - Tokenization and lowercasing\n",
    "     -  Removing Stopwords and punctuations , Stemming\n",
    "     \n",
    "    '''\n",
    "    \n",
    "    # Step 1\n",
    "    # Removeing handles, # , Urls andextra space \n",
    "    tweet = re.sub(r'@\\w+', '', tweet) # Remove handles\n",
    "    tweet= re.sub(r'https?://\\S+', '', tweet) # Remove URLs\n",
    "    tweet= re.sub(r'#\\w+', '', tweet) # Remove #\n",
    "    \n",
    "    # Optional: remove extra spaces that may have been left by removals\n",
    "    tweet = re.sub(r'\\s+', ' ',tweet).strip()\n",
    "    \n",
    "    #Step 2\n",
    "    #Tokenization\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "    tokenize_tweet = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    # Step 3 \n",
    "    # Removing stopwords ,  punctuations and stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    process_tweet = []\n",
    "    \n",
    "    for word in tokenize_tweet:\n",
    "        if(word not in english_stopwords and word not in string.punctuation):\n",
    "            p_tweet = stemmer.stem(word)\n",
    "            process_tweet.append(p_tweet)\n",
    "    \n",
    "    \n",
    "    return process_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e18bb35",
   "metadata": {},
   "source": [
    "### Step 3.2: Word Frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18bad254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement build freq counter\n",
    "\n",
    "def build_freqs(tweets , labels):\n",
    "    \n",
    "    freq_dict = {}\n",
    "    #Convert numpy array to list\n",
    "    y_labels = labels.squeeze().tolist()\n",
    "    for tweet , label in zip(tweets , y_labels):\n",
    "        p_tweet = process_tweet(tweet)\n",
    "        for word in p_tweet:\n",
    "            pair = (word , label)\n",
    "            freq_dict[pair] = freq_dict.get(pair,0) + 1\n",
    "            \n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9085c937",
   "metadata": {},
   "source": [
    "### Step 3.3: Sigmoid Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64f132e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement a sigmoid function \n",
    "\n",
    "def sigmoid(z):\n",
    "    \n",
    "    res = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73791ad1",
   "metadata": {},
   "source": [
    "### Step 3.4: Instances of feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2925831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a feature extraction\n",
    "\n",
    "def extract_feature(tweet ,  freqs , process_tweet = process_tweet):\n",
    "    \n",
    "    \n",
    "    X = np.zeros(3)\n",
    "    \n",
    "    X[0] = 1\n",
    "    \n",
    "    for word in process_tweet(tweet):\n",
    "        X[1] += freqs.get((word , 1.0) , 0)\n",
    "        X[2] += freqs.get((word , 0.0) , 0)\n",
    "   \n",
    "    return X\n",
    "\n",
    "# extract_feature(tweet_1  , freqs)\n",
    "\n",
    "# Implement a feature extractions\n",
    "\n",
    "def extract_features(tweets, freqs , extract_feature=extract_feature):\n",
    "    m = len(tweets)\n",
    "    \n",
    "    X = np.zeros((m,3))\n",
    "    \n",
    "    for i in range(m):\n",
    "        X[i,:] = extract_feature(tweets[i] , freqs)\n",
    "    \n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f06567",
   "metadata": {},
   "source": [
    "### Step 3.5: Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a501f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Descent \n",
    "def gradientDescent(X , Y , weights ,learning_rate,  num_iterations = 100):\n",
    " \n",
    "    m = len(X)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Calculate the prediction \n",
    "        Z = np.dot(X, weights)\n",
    "        \n",
    "        A = sigmoid(Z)\n",
    "        \n",
    "        #Calculate the cost function of the entire training set\n",
    "        epsilon = 1e-9  # Small value to avoid the warning of the divide by 0\n",
    "        cost = -1 / m * ((np.dot(Y.T , np.log(A + epsilon))) +  (np.dot((1 - Y).T , np.log(1-A +epsilon ))) )\n",
    "    \n",
    "        \n",
    "        #Calculate the gradient\n",
    "        dw = np.dot(X.T, (A-Y))\n",
    "        \n",
    "        #Update the weights\n",
    "        \n",
    "        weights = weights - learning_rate * dw\n",
    "    \n",
    "    \n",
    "    return  weights, cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e398aea4",
   "metadata": {},
   "source": [
    "### Step 3.6: Predict Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "535fdfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predict a tweet \n",
    "\n",
    "def predict_tweet(tweet, freqs , weights):\n",
    "    \n",
    "    # Extract feature of the tweet\n",
    "    \n",
    "    x = extract_feature(tweet , freqs)\n",
    "    \n",
    "    # Compute pred\n",
    "    z = np.dot(x , weights)\n",
    "    a = sigmoid(z)[0]\n",
    "    \n",
    "    if a >= 0.5:\n",
    "        return 1.0\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc903620",
   "metadata": {},
   "source": [
    "## Step 4 : Split the Data into Training and Test Sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e380b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two pieces , one for training (80%) and one for testing (20%)\n",
    "\n",
    "train_pos = all_positive_tweets[:4000]   # 4000 tweets\n",
    "train_neg = all_negative_tweets[:4000]   # 4000 tweets\n",
    "\n",
    "test_pos = all_positive_tweets[4000:]    # 1000 tweets\n",
    "test_neg = all_negative_tweets[4000:]    # 1000 tweets\n",
    "\n",
    "# X - train - 80 %\n",
    "train_x = train_pos + train_neg \n",
    "\n",
    "# X - test -  20% \n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "\n",
    "# Y - train\n",
    "train_y = np.append( np.ones(((len(train_pos)),1)) , np.zeros((len(train_neg),1)) , axis = 0)\n",
    "\n",
    "test_y = np.append( np.ones(((len(test_pos)),1)) , np.zeros((len(test_neg),1)) , axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acf3d8b",
   "metadata": {},
   "source": [
    "## Step 5 : Model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f38aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model (X_train, Y_train , X_test, Y_test ,   freqs , learning_rate , num_iteration=100):\n",
    "    \n",
    "    \n",
    "    #step 1: Feature extractions\n",
    "    X_train = extract_features(X_train , freqs)\n",
    "    \n",
    "    # setp 2: Gradient Descent for training\n",
    "#     initialize weights\n",
    "    weights = np.zeros((3,1))\n",
    "    weights , cost =  gradientDescent(X_train , Y_train , weights , learning_rate , num_iteration )\n",
    "    \n",
    "    \n",
    "    # Predict of the  test tweets\n",
    "    y_hat = []\n",
    "    for tweet in X_test:\n",
    "        y_pred = predict_tweet(tweet, freqs , weights)\n",
    "        y_hat.append(y_pred)\n",
    "    \n",
    "        \n",
    "    Y_test = Y_test.squeeze().tolist()\n",
    "    \n",
    "    total  = 0\n",
    "    \n",
    "    for i in range(len(y_hat)):\n",
    "        if y_hat[i] == Y_test[i]:\n",
    "            total += 1 \n",
    "            \n",
    "    # Calculate accuracy of the test examples\n",
    "    acc_test = (total / len(y_hat)) * 100\n",
    "    \n",
    "    res = {\n",
    "        \"costs\": cost,\n",
    "        \"Accuracy_test\": acc_test,\n",
    "        \"weights\": weights,\n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"num_iterations\": num_iteration\n",
    "        \n",
    "    }\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a40e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = build_freqs(train_x , train_y)\n",
    "\n",
    "\n",
    "model_eval = model(train_x ,  train_y , test_x , test_y,  freqs , 1e-9 ,1500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8055d0d",
   "metadata": {},
   "source": [
    "## STEP 6: Make predictions with an unseen tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91818ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model_eval[\"weights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1 = \"Joy in every moment! 🌟 #HappyLife\"\n",
    "tweet2 = \"Another letdown. 😞 #Frustrated\"\n",
    "tweet3 = \"Just what I needed, more rain. #PerfectDay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5369f2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_tweet(tweet1 ,  freqs , weights) # Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84721bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_tweet(tweet2 ,  freqs , weights) # Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90998745",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_tweet(tweet3 ,  freqs , weights) #  ????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c230f6",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "  <p>\n",
    "      Logistic regression is a classification model that assumes a linear relationship between input \n",
    "    features and the target variable. However, it does not inherently comprehend \n",
    "    language context or the order in which words occur. Subtle variations in sentiment, such as \n",
    "    negations or sarcasm, may be overlooked.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
